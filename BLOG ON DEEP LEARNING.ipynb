{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb88e38",
   "metadata": {},
   "source": [
    "# A.) Difference between ML and DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d8776",
   "metadata": {},
   "source": [
    "*  The main differences between Machine Learning (ML) and Deep Learning (DL) are:\n",
    "\n",
    "1 * Data representation: ML relies on manual feature engineering, while DL learns feature representations automatically from raw data.\n",
    "\n",
    "2 * Model complexity: ML models are typically simpler with fewer layers, while DL models are deep and can learn complex hierarchical representations.\n",
    "\n",
    "3 * Data requirements: ML models require curated and labeled data, while DL models excel with large amounts of raw, unstructured data.\n",
    "\n",
    "4 * Hardware requirements: DL models often need specialized hardware (e.g., GPUs) for efficient training, while ML models can often run on standard CPUs.\n",
    "\n",
    "5 * Application domains: ML is used in a wide range of applications, while DL has achieved great success in areas like computer vision and natural language processing.\n",
    "\n",
    "Overall, DL's strength lies in its ability to automatically learn complex patterns from raw data, whereas ML is still relevant for smaller datasets and more interpretable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6639675",
   "metadata": {},
   "source": [
    "# B) Brief history of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66890e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* 1943: The concept of artificial neural networks is introduced with the McCulloch-Pitts neuron model, laying the foundation for neural network research.\n",
    "\n",
    "* 1950s-1960s: Early neural network research takes place, with the development of the perceptron and the idea of gradient descent for learning.\n",
    "\n",
    "* 1980s-1990s: Neural networks face limitations and lose popularity due to challenges in training deep networks (known as the \"vanishing gradient\" problem) and the dominance of other machine learning techniques.\n",
    "\n",
    "* 2006: Geoff Hinton and his team revive interest in deep learning by introducing the concept of deep belief networks and developing new training algorithms.\n",
    "\n",
    "* 2012: Deep learning gains widespread attention when a deep convolutional neural network called AlexNet wins the ImageNet competition, dramatically improving image classification accuracy.\n",
    "\n",
    "* 2012-2015: Deep learning models, especially convolutional neural networks (CNNs) and recurrent neural networks (RNNs), achieve breakthroughs in various domains, including computer vision, natural language processing, and speech recognition.\n",
    "\n",
    "* 2014: Google's DeepMind develops a deep Q-network (DQN) that achieves human-level performance in playing Atari games, demonstrating the power of deep reinforcement learning.\n",
    "\n",
    "* 2015-2017: Deep learning continues to advance rapidly, with improvements in model architectures, training techniques, and the availability of large-scale labeled datasets. Deep learning models become the new state-of-the-art in many AI tasks.\n",
    "\n",
    "* 2018-present: Deep learning remains a dominant force in AI research and applications, with ongoing advancements in areas such as generative models (e.g., GANs), transfer learning, attention mechanisms, and self-supervised learning. Ethical considerations and interpretability of deep learning models also gain attention.\n",
    "\n",
    "Throughout this timeline, key contributions by researchers like Geoffrey Hinton, Yann LeCun, and Yoshua Bengio have played a crucial role in advancing deep learning and establishing it as a prominent field within AI. Deep learning has transformed various industries and continues to drive innovation and research in AI today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f319b9",
   "metadata": {},
   "source": [
    "# C) Explanation of MCp neuron in short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a524e888",
   "metadata": {},
   "source": [
    "* The MCP (McCulloch-Pitts) neuron is a simplified mathematical model of an artificial neuron, proposed by Warren McCulloch and Walter Pitts in 1943. It serves as the foundation for neural network research.\n",
    "\n",
    "* In the MCP neuron model, each input is associated with a weight. The neuron takes binary inputs and computes a weighted sum of these inputs. If the weighted sum exceeds a predefined threshold, the neuron \"fires\" and produces an output of 1; otherwise, it outputs 0. This binary threshold behavior is inspired by the firing behavior of biological neurons.\n",
    "\n",
    "* The MCP neuron is a basic building block that allows for simple computational operations. While it laid the groundwork for neural network development, it is not commonly used in modern deep learning architectures. More complex neuron models, such as sigmoid neurons or rectified linear units (ReLUs), are now widely used, enabling continuous outputs and more flexible nonlinear computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f2126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
